{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "notEEFMAAAAJ", "source": "AUTHOR_PROFILE_PAGE", "name": "Yunsheng Ma", "url_picture": "https://scholar.googleusercontent.com/citations?view_op=view_photo&user=notEEFMAAAAJ&citpid=5", "affiliation": "Purdue University", "organization": 9642744976452465328, "interests": ["üöó Autonomous Driving", "üëÅÔ∏è Computer Vision", "üí° Multimodal Learning"], "email_domain": "@purdue.edu", "homepage": "https://ysma.me/", "citedby": 589, "publications": {"notEEFMAAAAJ:roLk4NBRz8UC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "A Survey on Multimodal Large Language Models for Autonomous Driving", "pub_year": "2024"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:roLk4NBRz8UC", "num_citations": 161, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4362842677674220403", "cites_id": ["4362842677674220403"]}, "notEEFMAAAAJ:u5HHmVD_uO8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "An End-to-End Visual-Audio Attention Network for Emotion Recognition in User-Generated Videos", "pub_year": "2020"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:u5HHmVD_uO8C", "num_citations": 111, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14035781332178204168", "cites_id": ["14035781332178204168"]}, "notEEFMAAAAJ:QIV2ME_5wuYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Drive As You Speak: Enabling Human-Like Interaction With Large Language Models in Autonomous Vehicles", "pub_year": "2024"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:QIV2ME_5wuYC", "num_citations": 70, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6327080753062313282", "cites_id": ["6327080753062313282"]}, "notEEFMAAAAJ:WF5omc3nYNoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles", "pub_year": "2024"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:WF5omc3nYNoC", "num_citations": 48, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5409906793871363987,12549599450834453265", "cites_id": ["5409906793871363987", "12549599450834453265"]}, "notEEFMAAAAJ:YOwf2qJgpHMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Monocular Image Based 3D Model Retrieval", "pub_year": "2019"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:YOwf2qJgpHMC", "num_citations": 34, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14201399044020919218", "cites_id": ["14201399044020919218"]}, "notEEFMAAAAJ:IjCSPb-OGe4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Peer-to-Peer Federated Continual Learning for Naturalistic Driving Action Recognition", "pub_year": "2023"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:IjCSPb-OGe4C", "num_citations": 22, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=18237146093158783746", "cites_id": ["18237146093158783746"]}, "notEEFMAAAAJ:Se3iqnhoufwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs", "pub_year": "2024"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:Se3iqnhoufwC", "num_citations": 18, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7365596541269001341", "cites_id": ["7365596541269001341"]}, "notEEFMAAAAJ:ULOm3_A8WrAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Spurious Correlations in Machine Learning: A Survey", "pub_year": "2024"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:ULOm3_A8WrAC", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14180650253390850002", "cites_id": ["14180650253390850002"]}, "notEEFMAAAAJ:7PzlFSSx8tAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "REDFormer: Radar Enlightens the Darkness of Camera Perception with Transformers", "pub_year": "2023"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:7PzlFSSx8tAC", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6787901959466845304", "cites_id": ["6787901959466845304"]}, "notEEFMAAAAJ:zYLM7Y9cAGgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "CEMFormer: Learning to Predict Driver Intentions from In-Cabin and External Cameras via Spatial-Temporal Transformers", "pub_year": "2023"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:zYLM7Y9cAGgC", "num_citations": 13, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17827208101241675343", "cites_id": ["17827208101241675343"]}, "notEEFMAAAAJ:UebtZRa9Y70C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Large Language Models for Autonomous Driving: Real-World Experiments", "pub_year": "2023"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:UebtZRa9Y70C", "num_citations": 12, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2406531218319323144", "cites_id": ["2406531218319323144"]}, "notEEFMAAAAJ:9ZlFYXVOiuMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "M2DAR: Multi-View Multi-Scale Driver Action Recognition with Vision Transformer", "pub_year": "2023"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:9ZlFYXVOiuMC", "num_citations": 12, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14670566553153349694,14936306564603002300", "cites_id": ["14670566553153349694", "14936306564603002300"]}, "notEEFMAAAAJ:ufrVoPGSRksC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MACP: Efficient Model Adaptation for Cooperative Perception", "pub_year": "2024"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:ufrVoPGSRksC", "num_citations": 11, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2083233941548187279", "cites_id": ["2083233941548187279"]}, "notEEFMAAAAJ:3fE2CSJIrl8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "ViT-DD: Multi-Task Vision Transformer for Semi-Supervised Driver Distraction Detection", "pub_year": "2022"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:3fE2CSJIrl8C", "num_citations": 11, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16606137565155959834", "cites_id": ["16606137565155959834"]}, "notEEFMAAAAJ:L8Ckcad2t8MC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Driver Digital Twin for Online Recognition of Distracted Driving Behaviors", "pub_year": "2024"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:L8Ckcad2t8MC", "num_citations": 9, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14637724951985424138", "cites_id": ["14637724951985424138"]}, "notEEFMAAAAJ:kNdYIx-mwKoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Radar Enlighten the Dark: Enhancing Low-Visibility Perception for Automated Vehicles with Camera-Radar Fusion", "pub_year": "2023"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:kNdYIx-mwKoC", "num_citations": 9, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6427446718005473285", "cites_id": ["6427446718005473285"]}, "notEEFMAAAAJ:YsMSGLbcyi4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Mitigating Transformer Overconfidence via Lipschitz Regularization", "pub_year": "2023"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:YsMSGLbcyi4C", "num_citations": 9, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15203008009729655075", "cites_id": ["15203008009729655075"]}, "notEEFMAAAAJ:4DMP91E08xMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MM-SpuBench: Towards Better Understanding of Spurious Biases in Multimodal LLMs", "pub_year": "2024"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:4DMP91E08xMC", "num_citations": 3, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11832487464621624218", "cites_id": ["11832487464621624218"]}, "notEEFMAAAAJ:aqlVkmm33-oC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "What is the Visual Cognition Gap between Humans and Multimodal LLMs?", "pub_year": "2024"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:aqlVkmm33-oC", "num_citations": 3, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2717036321870461877", "cites_id": ["2717036321870461877"]}, "notEEFMAAAAJ:qxL8FJ1GzNcC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MAPLM: A Real-World Large-Scale Vision-Language Benchmark for Map and Traffic Scene Understanding", "pub_year": "2024"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:qxL8FJ1GzNcC", "num_citations": 3, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14551091005837653590,1416097965495507901", "cites_id": ["14551091005837653590", "1416097965495507901"]}, "notEEFMAAAAJ:4TOpqqG69KYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Quantifying Uncertainty in Motion Prediction with Variational Bayesian Mixture", "pub_year": "2024"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:4TOpqqG69KYC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13473117748183728243", "cites_id": ["13473117748183728243"]}, "notEEFMAAAAJ:dhFuZR0502QC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Video Token Sparsification for Efficient Multimodal LLMs in Autonomous Driving", "pub_year": "2024"}, "filled": false, "author_pub_id": "notEEFMAAAAJ:dhFuZR0502QC", "num_citations": 0}}, "citedby5y": 589, "hindex": 12, "hindex5y": 12, "i10index": 14, "i10index5y": 14, "cites_per_year": {"2020": 9, "2021": 22, "2022": 35, "2023": 94, "2024": 427}, "updated": "2024-10-18 08:02:21.597677"}